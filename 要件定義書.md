# Lightroom × ChatGPT 自動現像システム｜設計書 v1.0

---

## 0. 議論層の宣言
- **哲学層**: 設定は“感性の再現”ではなく“構造の記述”。意図→数値→再現性。
- **戦略層**: Lightroom Classic SDK（Lua）× ローカル橋渡し（Python/Node）× ChatGPT（JSON入出力）。
- **実務層**: プラグイン＋ブリッジ＋ジョブキューで、選択写真へ一括適用・プレビュー・ロールバック。
- **創造層**: 「作風プリセット」×「シーン診断」×「微調整ルール」の合成で、Junmai流の“白層（WhiteLayer）”等を定式化。

---

## 1. 目的・非目的
### 1.1 目的
- ChatGPTに与えた**構造化指示（JSON）**を、Lightroom Classicで**自動適用**し、**プレビュー→バッチ確定**できる仕組みを提供。
- **設定の再現性**と**差分管理**を担保し、作業を半自動化。

### 1.2 非目的
- Lightroom（クラウド版）APIによるクラウド制御（Classic優先）
- 自然言語の“曖昧な美辞”をそのまま数値化（必ずJSONへ落とす）

---

## 2. 想定環境・前提
- **Lightroom Classic** 12+（SDK/Lua使用）
- OS: Windows 10/11 または macOS 13+
- カタログ運用: LrC標準（プレビュー生成は既存運用を踏襲）
- ChatGPT（OpenAI API）へのアクセス（API Key）

> ※クラウド版LightroomはSDKが異なり制約が大きいため、本設計はClassic前提。

---

## 3. 全体アーキテクチャ
```
[ChatGPT] ⇄(JSON仕様)⇄ [ローカル・ブリッジ] ⇄(REST/ファイル監視)⇄ [LrC Luaプラグイン]
                                          │
                                          └→ [ジョブキュー／ログ／ロールバック管理]
```
- **ChatGPT**: “LrDevConfig v1”スキーマに準拠したJSONを生成
- **ローカル・ブリッジ**（Python/Flask もしくは Node/Express）:
  - JSONを受け取り**ローカル保存**（`jobs/`）
  - LrCプラグインと**双方向通信**（HTTPローカル or ファイル監視）
- **LrCプラグイン**（Lua）:
  - `LrDevelopController`で**スライダー適用**
  - **選択写真 / コレクション / フィルタ対象**へ一括適用
  - **仮想コピー作成**と**スナップショット**でロールバック可能に

---

## 4. データ仕様（LrDevConfig v1）
```json
{
  "version": "1.0",
  "target": {
    "scope": "selected|collection|smartCollection|folder|all",
    "identifier": "<id-or-name>",
    "createVirtualCopy": true
  },
  "pipeline": [
    {
      "stage": "base",
      "settings": {
        "WB": {"mode": "Custom", "temp": 4700, "tint": 8},
        "Exposure": -0.15,
        "Contrast": 6,
        "Highlights": -18,
        "Shadows": 12,
        "Whites": 8,
        "Blacks": -6,
        "Clarity": 6,
        "Dehaze": 2,
        "Vibrance": 8,
        "Saturation": -2
      }
    },
    {
      "stage": "toneCurve",
      "rgb": [[0,0],[28,22],[64,60],[190,192],[255,255]]
    },
    {
      "stage": "HSL",
      "hue": {"orange": -4, "yellow": -6},
      "sat": {"orange": -6, "blue": -8},
      "lum": {"orange": 4, "blue": -6}
    },
    {
      "stage": "detail",
      "sharpen": {"amount": 30, "radius": 1.0, "detail": 20, "masking": 70},
      "nr": {"luminance": 10, "color": 15}
    },
    {
      "stage": "effects",
      "grain": {"amount": 0},
      "vignette": {"amount": -6, "midpoint": 40, "roundness": 0, "feather": 60}
    },
    {
      "stage": "calibration",
      "redPrimary": {"hue": 2, "sat": -2},
      "greenPrimary": {"hue": 0, "sat": 0},
      "bluePrimary": {"hue": -2, "sat": 6}
    },
    {
      "stage": "local",
      "brushes": [
        {"name": "SkinSoft", "preset": "WhiteLayer_Skin_v2", "opacity": 70},
        {"name": "LipPop", "preset": "Lip_Pop_v1", "opacity": 40}
      ]
    },
    {
      "stage": "preset",
      "apply": ["WhiteLayer_Transparency_v4.xmp"],
      "blend": {"mode": "opacity", "amount": 60}
    }
  ],
  "export": {
    "enable": false,
    "preset": "Junmai_IG_2048_long_edge",
    "dest": "D:/export/2025-portraits"
  },
  "safety": {"snapshot": true, "dryRun": true}
}
```

- **ポイント**
  - `pipeline`は**順序保証**（例：ベース→曲線→HSL→ディテール→効果→キャリブ→ローカル→プリセット）
  - `preset.blend`は擬似。プラグイン側で**適用後に差分調整**で“ブレンド相当”を再現
  - `safety.dryRun`時は**プレビューのみ**（仮想コピー・スナップショット作成）

---

## 5. 処理フロー（標準）
1. **指示生成**: ChatGPTへプロンプト → JSON（LrDevConfig v1）を得る
2. **投入**: ブリッジGUIでJSONを貼付 or ファイル投入（`jobs/inbox/*.json`）
3. **キュー登録**: ジョブID採番・妥当性検証（スキーマ）
4. **適用前保全**: 仮想コピー作成＋スナップショット
5. **適用**: `LrDevelopController:setValue()` 等で順次適用
6. **検査**: ビュー切替/ズーム/比較モードで自動スクリーンショット保存（任意）
7. **確定/破棄**: dryRunなら**保留**、本番は**メタデータにJSON埋込（履歴）**
8. **ログ出力**: 失敗時は写真ID・段階・例外を記録

---

## 6. LrC プラグイン設計（Lua）
- **構成**
  - `JunmaiAutoDev.lrdevplugin/`
    - `Info.lua`（プラグイン定義）
    - `Main.lua`（起動・UI・監視）
    - `JobRunner.lua`（ジョブ処理）
    - `Stages/`（各stageの適用器）
    - `Utils/`（JSON、ログ、選択取得、スナップショット）
- **主要API**
  - `LrTasks`, `LrDialogs`, `LrView`, `LrApplication`, `LrDevelopController`, `LrHttp`（※送信のみ）
- **制約対策**
  - 受信サーバはLuaで建てない（SDK制限）。**ローカル・ブリッジ**が受信し、プラグインは**ポーリング** or **ファイル監視**

---

## 7. ローカル・ブリッジ設計（例：Python/Flask）
- エンドポイント
  - `POST /job` : JSON受信→`jobs/inbox/`保存
  - `GET /job/next` : 次ジョブ払い出し
  - `POST /job/<id>/result` : 実行結果受領
- 補助
  - スキーマ検証（`jsonschema`）
  - キューとリトライ（`sqlite` or `tinydb`）
  - 簡易GUI（PyQt/Tkinter）で**貼付→投入**も可能

---

## 8. 例：ChatGPTへのプロンプト雛形
```
被写体: 秋の屋外ポートレート（逆光）
質感: 透明感、肌は柔らかく、空色は転びすぎない
作風: WhiteLayer透明感系 v4 を60%相当
出力: LrDevConfig v1 JSON（日本語コメント不要）
制約: 肌の橙-4/彩度-6/輝度+4、空の青は彩度-8/輝度-6、トーンカーブはS弱
```

---

## 9. 運用フロー（バッチ）
1. コレクションに“_INBOX_2025-11-xx”を用意
2. 選択写真を投入
3. JSONを投げて**dryRun**で試適用→プレビュー確認
4. 問題なければ`dryRun=false`で本番適用
5. 仕上がりOKならエクスポートプリセットで書き出し

---

## 10. ロールバックと安全
- すべてのジョブ前に**仮想コピー**＋**スナップショット**
- 写真の`Metadata:Instructions`に**ジョブJSON埋込**（将来の再適用に備える）
- 失敗段階までの**逆適用**（スナップショット復帰）

---

## 11. ログ/監査
- `logs/yyyy-mm-dd.log`に**時刻/写真ID/ステージ/値/結果**
- 集計レポート（適用時間、失敗率、再実行件数）

---

## 12. 導入手順（初回）
1. OpenAI API Keyを**環境変数**へ設定
2. ブリッジ（Python/Node）をインストールし、起動
3. LrCプラグイン`JunmaiAutoDev.lrdevplugin`を追加→有効化
4. 設定で**ポーリング間隔**と**ジョブフォルダ**を指定
5. テストJSONを投入→dryRunで動作確認

---

## 13. 既知の制約
- LrC SDKは**受信サーバ不可** → ローカル・ブリッジ必須
- 一部ローカル調整（ブラシ等）は**完全自動が難しい** → プリセット適用＋不透明度相当のパラメータで近似
- プリセット“ブレンド”は**近似実装**（適用後に主要スライダーでウェイト調整）

---

## 14. 将来拡張
- 画像特性の**自動診断（顔/空/緑量）**でルール分岐
- **ホットフォルダ**監視 → 取り込み→自動現像→自動書き出し
- **作風バージョン管理**（v4.1, v4.2…）とABテスト

---

## 15. テスト計画（抜粋）
- スキーマ検証・異常系（欠落キー、値範囲外）
- 100枚バッチでの処理時間とメモリ
- 主要作風の再現差分（視覚評価＋数値差）

---

## 16. ミニ実装スニペット（概念）
**Lua（適用フレーム）**
```lua
local dc = import 'LrDevelopController'
dc.setValue('Exposure', -0.15)
dc.setValue('Highlights', -18)
-- ... 以降パイプライン順に適用
```

**Python（ブリッジ雛形）**
```python
from flask import Flask, request, jsonify
import uuid, json, pathlib
app = Flask(__name__)

@app.post('/job')
def job():
    j = request.get_json()
    jid = uuid.uuid4().hex
    pathlib.Path('jobs/inbox').mkdir(parents=True, exist_ok=True)
    open(f'jobs/inbox/{jid}.json','w',encoding='utf-8').write(json.dumps(j,ensure_ascii=False))
    return jsonify({"jobId": jid})
```

---

## 17. サンプル・プリセット対応表（例）
| 作風 | 実体 | 適用順 | 近似ブレンド | 備考 |
|---|---|---|---|---|
| WhiteLayer_Transparency_v4 | XMPプリセット | stage:preset末尾 | 60%相当 | 肌/青抑制は別stageで補正 |
| SkinSoft v2 | ローカルプリセット | stage:local | 不透明度70 | マスクは顔検出依存→準自動 |

---

## 18. 未確定事項（要回答）
1) **Lightroomの版**：Classicで確定で良いか？（クラウド版併用の有無）
2) **OS**：メイン環境は Windows / macOS のどちら？（ブリッジ実装最適化のため）

---

## 19. 無料化オプション（Gemini / OSS）
**方針A: Gemini API（無料枠活用）**
- モデル: `gemini-2.5-flash` または `gemini-2.5-flash-lite`
- 方式: **Structured Output**（JSON Schema）で `LrDevConfig v1` を強制生成
- 長所: 高精度・高速、セットアップが容易
- 留意: 無料枠内のRPM/RPD/TPMに収まるよう**ジョブキュー**で節流し（バッチ時は待機を許容）

**方針B: OSSローカルLLM（完全無料・オフライン）**
- ランタイム: **Ollama**／**LM Studio**／**llama.cpp サーバ**（いずれもローカルHTTP）
- モデル例: `Llama 3.x 8B/11B`, `Mixtral 8x7B`, `Qwen 7B/14B` 等（商用可のライセンスを選択）
- 方式: JSON Schema/GBNFで**構造化出力を強制**し `LrDevConfig v1` を返却
- 長所: コスト0・通信不要・大量バッチに強い（マシン性能依存）
- 留意: 推論速度と品質はGPU/CPU・量子化に依存

---

## 20. プロバイダ別アダプタ設計
```
interface ProviderAdapter {
  name: 'gemini' | 'ollama' | 'lmstudio' | 'llamacpp'
  generate_config(prompt: string, schema: object): Promise<LrDevConfig>
}
```

**共通ルール**
- すべて `schema = LrDevConfig v1 JSON Schema` を使用
- `temperature=0〜0.3`、`top_p<=0.9` で決定性を上げる
- 失敗時は**再試行**（最大3回）→それでも不正なら**バリデーションエラー**に落とす

**Gemini（Python例）**
```python
from google import genai
from pydantic import BaseModel

client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))
resp = client.models.generate_content(
  model='gemini-2.5-flash',
  contents=[{"role":"user","parts":[{"text":prompt}]}],
  config={
    "response_mime_type": "application/json",
    "response_schema": lrdevconfig_schema  # LrDevConfig v1 のJSON Schema
  }
)
config = resp.parsed  # 直接JSONとして取得
```

**Ollama（HTTP例）**
```bash
curl -X POST http://localhost:11434/api/chat \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "llama3.1:8b-instruct",
    "messages": [{"role":"user","content": "'""${PROMPT}""'"}],
    "stream": false,
    "format": LrDevConfigSchemaJson
  }'
```

**LM Studio（OpenAI互換例 / Node）**
```js
import OpenAI from 'openai';
const client = new OpenAI({ baseURL: 'http://localhost:1234/v1', apiKey: 'lm-studio' });
const resp = await client.responses.create({
  model: 'mixtral-8x7b-instruct',
  input: prompt,
  response_format: { type: 'json_schema', json_schema: LrDevConfigSchemaJson, strict: true }
});
```

**llama.cpp（GBNF/JSON Schema）**
- `grammar` または `json_schema` パラメータで強制
- 既存ブリッジの`/provider/llamacpp` アダプタでHTTP経由

---

## 21. 導入手順（無料ルート別）
**A. Gemini**
1) AI StudioでAPIキー取得 → 環境変数`GEMINI_API_KEY`へ
2) ブリッジの`provider=gemini`を有効化
3) レート制御（RPM/RPD）をジョブキュー側で設定

**B. OSS（推奨: Ollama）**
1) Ollamaを導入 → `ollama pull llama3.1:8b-instruct` 等
2) `http://localhost:11434/api/chat` に対しJSON Schemaを`format`に渡す
3) ブリッジの`provider=ollama`を有効化

**C. OSS（LM Studio）**
1) LM Studioを導入 → モデル読込 → **OpenAI互換API**をON
2) ブリッジの`baseURL`を`http://localhost:1234/v1`へ

---

## 22. スキーマ（LrDevConfig v1）補足
- `enum`・`required`・数値範囲（`minimum/maximum`）で**厳格化**
- `pipeline[].stage`は `base|toneCurve|HSL|detail|effects|calibration|local|preset` に限定
- バリデーションNG → 自動再試行→失敗時は**人手確認キュー**へ

---

## 23. 既存セクションの更新
- **2. 想定環境**に「Gemini API または OSSローカルLLM（Ollama/LM Studio/llama.cpp）」を追加
- **3. 全体アーキテクチャ**に「Provider Adapter」層を追加
- **7. ブリッジ設計**に `/provider/*` ルータ（`gemini|ollama|lmstudio|llamacpp`）を追加

---

## 24. 未確定事項（追加）
3) **無料ルートの第一選択**: A) Gemini API / B) OSS（Ollama か LM Studio）
4) **ローカルGPU**: GPU/VRAM容量（8–24GB）と希望モデル（例: Llama3 8B or Mixtral）

---

以上。無料ルート（Gemini/OSS）に最適化した設計へ拡張済み。ここから雛形コードを固められます。
